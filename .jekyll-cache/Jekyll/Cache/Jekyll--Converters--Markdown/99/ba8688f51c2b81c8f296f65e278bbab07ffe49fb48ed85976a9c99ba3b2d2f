I"N<p>Today a new journal article lead by o2r team member <a href="https://nordholmen.net/">Daniel</a> was published in the journal <a href="https://peerj.com/">PeerJ</a>:</p>

<p><img src="https://img.shields.io/badge/article-peer--reviewed-brightgreen.svg" alt="publication badge" class="publicationBadge" /><strong><a href="https://doi.org/10.7717/peerj.5072" title="CRIS entry of publication">Reproducible research and GIScience: an evaluation using AGILE conference papers</a></strong> by <i><a href="https://orcid.org/0000-0002-0024-5046">Daniel Nüst</a>, <a href="https://orcid.org/0000-0003-1004-9695">Carlos Granell</a>, <a href="https://orcid.org/0000-0001-7078-3766">Barbara Hofer</a>, <a href="https://orcid.org/0000-0001-6651-0976">Markus Konkol</a>, <a href="https://orcid.org/0000-0002-9317-8291">Frank O. Ostermann</a>, <a href="https://orcid.org/0000-0002-8245-3016">Rusne Sileryte</a>, <a href="https://orcid.org/0000-0002-9612-1581">Valentina Cerutti</a></i>
<br />
<i class="editor">PeerJ. 2018.</i><strong>doi: <a href="https://doi.org/10.7717/peerj.5072">10.7717/peerj.5072</a></strong></p>

<p>The article is an outcome of a collaboration around the AGILE conference, see <a href="https://o2r.info/reproducible-agile/">https://o2r.info/reproducible-agile/</a> for more information.
Please <a href="https://twitter.com/f_ostermann/status/1017673264334766080">retweet</a> and spread the word!
<a href="https://peerj.com/articles/5072/#questions">Your questions &amp; feedback</a> are most welcome.</p>

<p>Here is Daniel’s attempt at a <strong><a href="https://twitter.com/Protohedgehog/status/949315968903376896">non-specialist summary</a></strong>:</p>

<blockquote>
  <p>More and more research use data and algorithms to answer a question.
That makes it harder for researchers to understand a scientific publication, because you need more than just the text to understand what is really going on.
You need the software and the data to be able to tell if everything is done correctly, and to be able to re-use new and exciting methods.
We took a look at the existing guides for such research and created our own criteria for research in sciences using environmental observations and maps.
We used the criteria to test how reproducible a set of papers from the AGILE conference actually are.
The conference is quite established and the papers are of high quality because they were all suggested for the “best paper” awards at the conference.</p>

  <p>The results are quite bad!
We could not re-create any of the analyses.
Then we asked the authors of the papers we evaluated if they had considered that someone else might want to re-do their work.
While they all think the idea is great, many said they do not have the time for it.</p>

  <p>The only way for researchers to have the time and resources to work in a way that is transparent to others and reusable openly is either to convince them of the importance or to force them.
We came up with a list of suggestions to publishers and scientific conference organisers to create enough reasons for researchers to publish science in a re-creatable way.</p>
</blockquote>
:ET