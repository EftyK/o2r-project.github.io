---
layout: post
title: "New article published in PeerJ"
categories:
  - AGILE
  - conference
  - article
  - PeerJ
author: 'Daniel Nüst'
disable_excerpt: true
---

Today a new journal article lead by o2r team member [Daniel](https://nordholmen.net/) was published in the journal [PeerJ](https://peerj.com/):

<img src="https://img.shields.io/badge/article-peer--reviewed-brightgreen.svg" alt="publication badge" class="publicationBadge"><strong><a href="https://doi.org/10.7717/peerj.5072" title="CRIS entry of publication">Reproducible research and GIScience: an evaluation using AGILE conference papers</a></strong> by <i>[Daniel Nüst](https://orcid.org/0000-0002-0024-5046), [Carlos Granell](https://orcid.org/0000-0003-1004-9695), [Barbara Hofer](https://orcid.org/0000-0001-7078-3766), [Markus Konkol](https://orcid.org/0000-0001-6651-0976), [Frank O. Ostermann](https://orcid.org/0000-0002-9317-8291), [Rusne Sileryte](https://orcid.org/0000-0002-8245-3016), [Valentina Cerutti](https://orcid.org/0000-0002-9612-1581)</i>
<br>
<i class="editor">PeerJ. 2018.</i><strong>doi:&nbsp;<a href="https://doi.org/10.7717/peerj.5072">10.7717/peerj.5072</a></strong>

The article is an outcome of a collaboration around a at the AGILE conference, see [http://o2r.info/reproducible-agile/](http://o2r.info/reproducible-agile/) for more information.
Please [retweet](https://twitter.com/f_ostermann/status/1017673264334766080) and spread the word!
[Your questions & feedback](https://peerj.com/articles/5072/#questions) are most welcome.

Here is Daniel's attempt at a **[non-specialist summary](ttps://twitter.com/Protohedgehog/status/949315968903376896)**:

> More and more research use data and algorithms to answer a question.
> That makes it harder for researchers to understand a scientific publication, because you need more than just the text to understand what is really going on.
> You need the software and the data to be able to tell if everything is done correctly, and to be able to re-use new and exciting methods.
> We took a look at the existing guides for such research and created our own criteria for research in sciences using environmental observations and maps.
> We used the criteria to test how reproducible a set of papers from the AGILE conference actually are.
> The conference is quite established and the papers are of high quality because they were all suggested for the "best paper" awards at the conference.
>
> The results are quite bad!
> We could not re-create any of the analyses.
> Then we asked the authors of the papers we evaluated if they had considered that someone else might want to re-do their work.
> While they all think the idea is great, many said they do not have the time for it.
>
> The only way for researchers to have the time and resources to work in a way that is transparent to others and reusable openly is either to convince them of the importance or to force them.
> We came up with a list of suggestions to publishers and scientific conference organisers to create enough reasons for researchers to publish science in a re-creatable way.
